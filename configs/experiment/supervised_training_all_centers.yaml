# @package _global_

defaults: 
  - override /datamodule: supervised_allcenters
  - override /model: supervised
  - override /callbacks: finetuner_callbacks_paul.yaml
  - override /trainer: exact_trainer.yaml
  - override /logger: wandb


trainer:
  min_epochs: 100
  max_epochs: 100
  gradient_clip_val: null
  limit_train_batches: null
  limit_val_batches: null


#model:
#  #loss_weights: 
#  #  _target_: torch.tensor
#  #  data: [1., 10.]
#  #  device: cuda

